{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4dcced1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from onnx2torch import convert\n",
    "from PIL import Image\n",
    "from PIL import ImageFont\n",
    "from PIL import ImageDraw \n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "485b0789",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(r\"C:\\Users\\PC\\Documents\\projects\\carPlates\\data\\letters420\\letter_labels.csv\")\n",
    "lst = []\n",
    "for i in range(len(df)):\n",
    "    if df['label'][i] == '0':\n",
    "        lst.append(i)\n",
    "df = df.drop(lst, axis = 0)\n",
    "s = list(set(df['label']))\n",
    "s = sorted(s)\n",
    "dic = {i:s[i] for i in range(len(s))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3ba92d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/ultralytics/yolov5/archive/master.zip\" to C:\\Users\\PC/.cache\\torch\\hub\\master.zip\n",
      "YOLOv5  2022-7-13 Python-3.9.12 torch-1.11.0 CUDA:0 (NVIDIA GeForce RTX 3060, 12288MiB)\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7012822 parameters, 0 gradients, 15.8 GFLOPs\n",
      "Adding AutoShape... \n",
      "Downloading: \"https://github.com/ultralytics/yolov5/archive/master.zip\" to C:\\Users\\PC/.cache\\torch\\hub\\master.zip\n",
      "YOLOv5  2022-7-13 Python-3.9.12 torch-1.11.0 CUDA:0 (NVIDIA GeForce RTX 3060, 12288MiB)\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5n summary: 213 layers, 1760518 parameters, 0 gradients, 4.1 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "plates_model_dir = \"trained_weights/plates/plates320_s.pt\"\n",
    "letters_model_dir = \"trained_weights/letters/letters320_n.pt\"\n",
    "#letters_model_dir = r\"C:\\Users\\PC\\Documents\\projects\\carPlates\\yolov5\\runs\\train\\exp35\\weights\\best.pt\"\n",
    "plates_model = torch.hub.load('ultralytics/yolov5', 'custom', path=plates_model_dir, force_reload=True)\n",
    "letters_model = torch.hub.load('ultralytics/yolov5', 'custom', path=letters_model_dir, force_reload=True)\n",
    "# Path to ONNX model\n",
    "onnx_model_path = r\"C:\\Users\\PC\\Documents\\projects\\carPlates\\yolov5\\trained_weights\\classification\\letter_model_mod_1.onnx\"\n",
    "# You can pass the path to the onnx model to convert it or...\n",
    "class_model = convert(onnx_model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46cb7d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = r\"E:\\DataSets\\carPlatesDataset\\cars_dataset\\images\\train\"\n",
    "raw_path = r\"E:\\DataSets\\carPlatesDataset\\raw\"\n",
    "\n",
    "train_imgs = os.listdir(train_path)\n",
    "raw_imgs = os.listdir(raw_path)\n",
    "test_imgs = []\n",
    "for img in raw_imgs:\n",
    "    if img not in train_imgs:\n",
    "        test_imgs.append(img)\n",
    "\n",
    "test_dirs = [os.path.join(raw_path, img) for img in test_imgs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc33c05b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[2.35624e+03, 6.98847e+02, 2.57528e+03, 8.10439e+02, 9.22770e-01, 0.00000e+00],\n",
      "        [2.87990e+03, 5.15233e+02, 3.01204e+03, 5.81613e+02, 9.20639e-01, 0.00000e+00],\n",
      "        [3.37030e+03, 4.28701e+02, 3.47018e+03, 4.84572e+02, 8.85897e-01, 0.00000e+00],\n",
      "        [1.63678e+03, 4.09884e+02, 1.72014e+03, 4.52904e+02, 8.53340e-01, 0.00000e+00],\n",
      "        [9.76102e+02, 1.08610e+03, 1.10961e+03, 1.24089e+03, 7.99172e-01, 0.00000e+00],\n",
      "        [2.12340e+03, 2.75564e+02, 2.16455e+03, 2.98308e+02, 3.29551e-01, 0.00000e+00]], device='cuda:0')]\n",
      "[[2356  698 2575  810    0    0]\n",
      " [2879  515 3012  581    0    0]\n",
      " [3370  428 3470  484    0    0]\n",
      " [1636  409 1720  452    0    0]\n",
      " [ 976 1086 1109 1240    0    0]\n",
      " [2123  275 2164  298    0    0]]\n"
     ]
    }
   ],
   "source": [
    "img_dir = test_dirs[1]\n",
    "t1 = time.time()\n",
    "img = Image.open(img_dir)\n",
    "plates_pred = plates_model(img)\n",
    "print(plates_pred.xyxy)\n",
    "m = torch.Tensor.cpu(plates_pred.xyxy[0])\n",
    "if m[0][4] < 0.5:\n",
    "    print(\"yes\")\n",
    "m = np.array(m, dtype = np.int64)\n",
    "img = np.array(img, dtype = np.uint8)\n",
    "plate = Image.fromarray(img[m[1][1]:m[1][3], m[1][0]:m[1][2]])\n",
    "plate.show()\n",
    "print(m)\n",
    "#plate = Image.fromarray(img[int(m[1][0]): int(m[1][2]), int(m[1][1]):int(m[1][3])])\n",
    "#plate.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "296c4d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bin(img):\n",
    "    \"\"\"mx, mn = (img[0][0], img[0][0])\n",
    "    for row in img:\n",
    "        rmax = max(row)\n",
    "        rmin = min(row)\n",
    "        if rmax > mx:\n",
    "            mx = rmax\n",
    "        if rmin < mn:\n",
    "            mn = rmin\n",
    "    new_img = []\n",
    "    mn = int(mn)\n",
    "    mx = int(mx)\n",
    "    thresh = (mn + mx)/2\"\"\"\n",
    "    new_img = []\n",
    "    thresh = 125.5\n",
    "    for row in img:\n",
    "        nrow = []\n",
    "        for pixel in row:\n",
    "            if pixel >= thresh:\n",
    "                nrow.append(255)\n",
    "            else:\n",
    "                nrow.append(0)\n",
    "        new_img.append(nrow)\n",
    "    return np.array(new_img)\n",
    "\n",
    "def get_bin(img):\n",
    "    (thresh, im_bw) = cv2.threshold(img, 125, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)\n",
    "    return im_bw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ead96c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def put_letters_together(lst):\n",
    "    output = \"\"\n",
    "    for letter in lst:\n",
    "        output += letter + \" \"\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68810e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rotated_img(img):\n",
    "        img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        edges = cv2.Canny(img_gray, 100, 200, None, 3)\n",
    "        lines = cv2.HoughLines(edges, 1, np.pi / 180, 100)\n",
    "        total_theta = []\n",
    "        if lines is not None:\n",
    "            for i in range(len(lines)):\n",
    "                if (abs(lines[i][0][1]) < 0.45 or abs(lines[i][0][1] - np.pi) < 0.45):\n",
    "                    continue\n",
    "                else:\n",
    "                    total_theta.append(lines[i][0][1])\n",
    "            if len(total_theta) != 0:\n",
    "                avg_theta = sum([theta for theta in total_theta])/len(total_theta) * (180/np.pi) - 90\n",
    "            else:\n",
    "                avg_theta = 0\n",
    "            (h, w) = img.shape[:2]\n",
    "            (cX, cY) = (w // 2, h // 2)\n",
    "            M = cv2.getRotationMatrix2D((cX, cY), avg_theta, 1.0)\n",
    "            rotated = cv2.warpAffine(img, M, (w, h))\n",
    "            return rotated\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72674b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cropped_img(img, percentage = 0.385):\n",
    "    return img[int(percentage*len(img[:, 0])):, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c33161a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mod_img(img, thresh = 75):\n",
    "    thresh = 75\n",
    "    new_img = img\n",
    "    pixels = np.where((new_img[:, :, 0] <= thresh), (new_img[:, :, 1] <= thresh), (new_img[:, :, 2] <= thresh))\n",
    "    blacks = np.where((new_img[:, :, 0] <= thresh) & (new_img[:, :, 1] <= thresh) & (new_img[:, :, 2] <= thresh))\n",
    "    new_img[blacks] = [0]\n",
    "    new_img[pixels] = [0]\n",
    "    return new_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "01cb785f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "407"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_dirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ee35d59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_pred(img):\n",
    "    thresh = 0.5\n",
    "    t1 = time.time()\n",
    "    plates_pred = plates_model(img)\n",
    "    plates = torch.Tensor.cpu(plates_pred.xyxy[0])\n",
    "    all_plates = []\n",
    "    final_img = np.copy(img)\n",
    "    for plate in plates:\n",
    "        if plate[4] < thresh:\n",
    "            continue\n",
    "        plate_reading = []\n",
    "        plate = np.array(plate, dtype = np.int64)\n",
    "        plate_img = get_rotated_img(img[plate[1]:plate[3], plate[0]:plate[2]])\n",
    "        plate_img = get_cropped_img(plate_img)\n",
    "        #plate_img_mod = get_mod_img(plate_img)\n",
    "        letter_pred = letters_model(plate_img)\n",
    "        letters = torch.Tensor.cpu(letter_pred.xyxy[0])\n",
    "        final_img = cv2.rectangle(final_img, (plate[0], plate[1]), (plate[2], plate[3]), (255, 0, 0), 2)\n",
    "        x_vals = []\n",
    "        for letter in letters:\n",
    "            if letter[4] < thresh:\n",
    "                continue\n",
    "            letter = np.array(letter, dtype = np.int64)\n",
    "            x_vals.append(letter[2])\n",
    "            letter_img = plate_img[letter[1]:letter[3], letter[0]:letter[2]]\n",
    "            letter_img = cv2.cvtColor(letter_img, cv2.COLOR_RGB2GRAY)\n",
    "            letter_img = cv2.resize(letter_img, (96, 96))\n",
    "            letter_img = get_bin(letter_img)/255\n",
    "            letter_img = torch.from_numpy(letter_img)\n",
    "            letter_img = letter_img.type(torch.FloatTensor)\n",
    "            class_pred = torch.Tensor.cpu(class_model(letter_img)).tolist()\n",
    "            class_pred = dic[np.argmax(class_pred)]\n",
    "            if (letter[0] < plate_img.shape[1]/2):\n",
    "                if class_pred == 'ه' or class_pred == 'د':\n",
    "                    class_pred = '5'\n",
    "                elif class_pred == 'ا' or class_pred == 'م':\n",
    "                    class_pred = '1'\n",
    "            plate_reading.append(class_pred)\n",
    "        plate_reading = [num for _, num in sorted(zip(x_vals, plate_reading))]\n",
    "        #plate_reading.reverse()\n",
    "        if plate_reading != []:\n",
    "            all_plates.append([plate_reading, (plate[0], plate[1]), int((plate[3] - plate[1])/3)])\n",
    "    final_img = Image.fromarray(final_img)\n",
    "    fontpath = r\"C:\\Users\\PC\\Documents\\projects\\carPlates\\font\\arial.ttf\"\n",
    "    draw = ImageDraw.Draw(final_img)\n",
    "    for plate in all_plates:\n",
    "        font = ImageFont.truetype(fontpath, plate[2])\n",
    "        if plate[0] != []:\n",
    "            plate_num = put_letters_together(plate[0])\n",
    "        draw.text(plate[1], plate_num, font = font, fill = (0, 255, 0))\n",
    "    font = ImageFont.truetype(fontpath, 28)\n",
    "    t2 = time.time()\n",
    "    draw.text((0, 0),   f\"Time for processing is {int((t2 - t1)*100000) / 100000} second\", font = font)\n",
    "    return np.array(final_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d34bae",
   "metadata": {},
   "source": [
    "for path in test_dirs[3:]:\n",
    "    i = len(os.listdir(\"predictions\"))\n",
    "    img_path = r\"C:\\Users\\PC\\OneDrive\\Desktop\\test16.jpg\"\n",
    "    img_path = test_dirs[3]\n",
    "    img_path = path\n",
    "    img = Image.open(img_path)\n",
    "    img = img_pred(np.array(img))\n",
    "    img = Image.fromarray(img)\n",
    "    img.save(f\"predictions/{i}.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "41bfcb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = test_dirs[10]\n",
    "img = Image.open(img_path)\n",
    "img = img_pred(np.array(img))\n",
    "img = Image.fromarray(img)\n",
    "img.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616882cb",
   "metadata": {},
   "source": [
    "import cv2\n",
    "\n",
    "vid = cv2.VideoCapture(0)\n",
    "  \n",
    "while(True):\n",
    "      \n",
    "    # Capture the video frame\n",
    "    # by frame\n",
    "    ret, frame = vid.read()\n",
    "    frame = img_pred(frame)\n",
    "    # Display the resulting frame\n",
    "    cv2.imshow('frame', frame)\n",
    "    # the 'q' button is set as the\n",
    "    # quitting button you may use any\n",
    "    # desired button of your choice\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "vid.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7b8071f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "   \n",
    "# Create a VideoCapture object and read from input file\n",
    "cap = cv2.VideoCapture(r\"E:\\DataSets\\carPlatesDataset\\raw_vids\\VID_20220712_205303.mp4\")\n",
    "   \n",
    "# Check if camera opened successfully\n",
    "if (cap.isOpened()== False): \n",
    "    print(\"Error opening video  file\")\n",
    "    \n",
    "# Read until video is completed\n",
    "while(cap.isOpened()):    \n",
    "  # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "    if ret == True:\n",
    "        frame = img_pred(frame)\n",
    "        cv2.imshow('Frame', frame)\n",
    "   \n",
    "    # Press Q on keyboard to  exit\n",
    "    if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "        break\n",
    "   \n",
    "  # Break the loop\n",
    "    #else: \n",
    "        #print(\"yes\")\n",
    "        #break\n",
    "        \n",
    "# When everything done, release \n",
    "# the video capture object\n",
    "cap.release()\n",
    "   \n",
    "# Closes all the frames\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb76c08",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
